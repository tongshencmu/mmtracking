#!/bin/bash
#SBATCH -N 1
#SBATCH -o stark_lasot.log
#SBATCH --partition=GPU-shared
#SBATCH --ntasks-per-node 8
#SBATCH --gres=gpu:v100-32:4
#SBATCH --time=48:00:00
#SBATCH --dependency=singleton

module load anaconda3
module load cuda
module load cudnn

# change 5-digit MASTER_PORT as you wish, slurm will raise Error if duplicated with others
export MASTER_PORT=23561
# WORLD_SIZE as gpus/node * num_nodes
export WORLD_SIZE=4

# echo "NODELIST="${SLURM_NODELIST}
# master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR="127.0.1.1"
# echo "MASTER_ADDR="$MASTER_ADDR

conda deactivate
conda activate /ocean/projects/ele220002p/tongshen/env/3d
srun ./tools/dist_train.sh configs/sot/stark/custom_stark_lasot.py 4